{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigml.api import BigML\n",
    "api = BigML(project='project/5d94a426eba31d46690001cd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources and datasets creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_full_dataset' (dict)\n",
      "Stored 'train_training_dataset' (dict)\n"
     ]
    }
   ],
   "source": [
    "train_full_source = api.create_source('./datasets/train_full_df.csv')\n",
    "api.ok(train_full_source) # waits the OK response from BigML server\n",
    "\n",
    "train_full_dataset = api.create_dataset(train_full_source)\n",
    "api.ok(train_full_dataset)\n",
    "%store train_full_dataset # stores the python object for other notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the *train_full_dataset* into a **80/20 random split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_validation_dataset' (dict)\n"
     ]
    }
   ],
   "source": [
    "train_training_dataset = api.create_dataset(train_full_dataset, {\"name\": \"train | training\", \"sample_rate\": 0.8, \"seed\": \"myseed\"})\n",
    "                                                                # \"seed\" is the hash string used for randomisation\n",
    "api.ok(train_training_dataset)\n",
    "%store train_training_dataset\n",
    "\n",
    "train_validation_dataset = api.create_dataset(\n",
    "    train_full_dataset, {\"name\": \"train | validation\",\n",
    "                        \"sample_rate\": 0.8, \"out_of_bag\": True, \"seed\": \"myseed\"})\n",
    "                         # \"out_of_bag\" takes the remaining 20% data from the previous split, \"seed\" matching the previous hash string\n",
    "api.ok(train_validation_dataset)\n",
    "%store train_validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the Kaggle test source and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kaggle_test_dataset' (dict)\n"
     ]
    }
   ],
   "source": [
    "kaggle_test_source = api.create_source('./datasets/test_df.csv')\n",
    "api.ok(kaggle_test_source)\n",
    "\n",
    "kaggle_test_dataset = api.create_dataset(kaggle_test_source)\n",
    "api.ok(kaggle_test_dataset)\n",
    "%store kaggle_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our train_training dataset into 10 ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('datasets_id.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Name\", \"dataset_id\"])\n",
    "\n",
    "def train_training_split(rate):\n",
    "    dataset = api.create_dataset(train_training_dataset, \n",
    "                                 {\"name\": f\"train | training | {rate}\", \"sample_rate\": rate, \"seed\": \"myseed\"})\n",
    "    api.ok(dataset)\n",
    "    with open('splitted_datasets.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([f\"train | training | {rate}\", dataset['resource']])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_training_dataset = api.get_dataset(\"dataset/5db2c2ebe47684746800a438\")\n",
    "for n in range(1, 10):\n",
    "    train_training_split(n/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
